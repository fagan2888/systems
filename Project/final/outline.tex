\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts}
\usepackage{graphicx}
\usepackage[]{algorithm2e}
\usepackage{subcaption}

\DeclareMathOperator*{\argmin}{arg\,min}
\providecommand{\m}[1]{\mathbf{#1}}
\providecommand{\norm}[1]{\left\|#1\right\|}
\usepackage{abstract}
\renewcommand{\abstractname}{}    % clear the title
\renewcommand{\absnamepos}{empty} % originally center

\addtolength{\textheight}{1in}
\addtolength{\voffset}{-1in}

\setlength{\textwidth}{5in}
\begin{document}
\title{Auto Scaling Online Learning}
\author{Marco Tulio Ribeiro, Shrainik Jain}
\renewcommand{\today}{Mar 17, 2014}
\maketitle

\begin{abstract}
We propose a framework and algorithms for scaling online machine learning up
or down, according to demand, and the priorities of the system. Different
systems have different needs in terms of the cost assigned to machines, the cost
of a bad user experience, etc. In this project, we focus most on the part that
is general to auto scaling any application (and not only machine learning
algorithms), but propose a framework that takes some ML particularities into
account.
\end{abstract}


\section{Introduction}
Online machine learning algorithms operate on a single instance at a time. They
have become particularly popular in natural language processing and applications
with streaming data, including classification, ranking, etc
\cite{Bordes:2005:HSE:2130928.2130979,
Carvalho:2006:SOL:1150402.1150466, Dredze:2008:CLC:1390156.1390190}. Online
learning is particularly interesting in the scenarios where data keeps streaming
in, such as a web search engine doing advertisement placement. It is also
interesting for scenarios where the whole dataset is too large to fit in main
memory, as online learning only operates on a single example at a time.

A lot of tasks that use online learning have a particular structure that can be 
broken down into 2 major components: 1 - Learning a model from data, and 2 -
making predictions according to the model. Going back to the web search engine
scenario as an example: the system needs to make predictions for every user
doing a query - and must also learn from the feedback given by those users. We
call machines that make predictions \textbf{predictors}, and machines that learn
from the feedback \textbf{learners}.

This structure comes with multiple challenges. First, the 
the amount of data is always growing, so archiving it comes at a cost, both
because of storage constraints and computation constraints. A solution to this
is to just keep the current model in memory, and archive the rest in the
background. A second challenge is the
variable speed at which data streams in. Imagine a learning problem where in the
learning dataset is a live twitter stream for a hashtag. In this scenario the
rate at which the data comes in is a function of the popularity of the hashtag.
Finally, different applications have different costs for learning, and different
requirements for the latency of predictions.

The problem we tackled in this project is the problem of automatically
handling the resources needed for online learning. The ideal system would
allocate the resources necessary to keep the prediction latency acceptable,
while at the same time learning appropriately. Finally, the system would be able
to handle bursts (such as increase in demand) and different learning
requirements for different systems. Since online learning in a distributed
system is a research problem on its \cite{pserver1, pserver2}, we abstracted
this part from our work, and focused on some of the systems challenges.

Current works in auto-scaling\cite{Mao:2011:AMC:2063384.2063449,6008748} address
many of the issues we address in this report (such as load prediction, framing
the problem as a cost minimization problem, etc). However, one aspect that we
thought was lacking in current work (at least from the papers we read) is
dealing with node failures and uncertainty. We reformulate the cost function in
terms of expected cost, in order to account for uncertainty pertaining future
load predictions and node failures. Finally, we evaluate some baselines and our
proposed approach with simulated loads.

\section{Architecture}
\begin{figure}[h!]
\center
\includegraphics[width=1.0\textwidth]{architecture.png}
\caption{Architecture: Node asks the load balancer for a predictor, and then
asks the predictor for a prediction.}
\label{architecture}
\end{figure}

We assume an architecture similar to the one presented in Figure
\ref{architecture}. There is a set of nodes acting as a Load Balancer
(whose addresses are known to the clients of the machine learning service). When
a client needs a predictor or a learner, it first requests an a node from the
Load Balancer, and then it proceeds to make the request. In Figure
\ref{architecture}, the client got assigned the node in red.

The Load Balancer reads the state of the current system from some distributed,
reliable storage. The state is comprised of which nodes are up and acting as
learners or predictors, and a measure of the \textbf{power} of each node - that
is, how many requests each node can effectively handle in a time interval. This
allows for load balancing even when heterogeneous nodes are present.

The state of the current system is modified every so often by the set of nodes
denoted \textbf{Autoscaler} in Figure \ref{architecture}. The Autoscaler
periodically sends heartbeat messages to nodes and to the load balancer, and
tests the prediction / learning time. It also gets the status - i.e., the number
of requests served, eventual node failures, etc.  Finally, it then updates the
state in the distributed storage according to some policy. This architecture is
very straightforward, and can be used with commodity machines for handling a lot
of requests, while still being resilient to failures (depending of course on the
Autoscaler policy).

\section{Auto Scaling}
\subsection{Objective}
The problem of auto scaling online learning can be formulated as minimizing a
cost function over a sequence of time intervals. This function for each time
interval is presented in
Equation 1. 
\begin{equation}
cost = \alpha SLA + \beta P + \gamma LR + \delta L
\end{equation}
\textbf{SLA} is an indicator variable that is positive when a Service License
Agreement (SLA) is violated in the time interval in question. This SLA is
related to predictions, and can be defined as a threshold on the average time
per prediction, on the maximum time per prediction, etc. There is a cost
$\alpha$ associated with the SLA violation, that depends on the specific
application that depends on the system. A bad user experience due to slow
page loading on a search engine can be fatal to the success of the business,
while a slower prediction for some internal testing service can be less costly.
\textbf{P} denotes the number of predictors active in the time interval, while
$\beta$ is a measure of the cost of each predictor. \textbf{LR} is a measure of
the learning rate achieved by the machine learning algorithm. Some of the most
popular online learning algorithms, such as Stochastic Gradient Descent, have a
property that error $\epsilon$ decreases
linearly with the number of training examples
\cite{Nemirovski:2009:RSA:1654243.1654247}: that is, the relation is $O(1 /
\epsilon)$ , meaning that if training examples
are available, adding more learners would offset the training error, and thus
increase whatever measure of Learning Rate is being used - although probably not with
linear speedup \cite{pserver1, pserver2}.  $\textbf{L}$ denotes
the number of learners active in the time interval.

This objective formulation decouples learning from predicting, which brings
advantages and disadvantages. Usually, the processor / memory requirements for
learning and predicting are different - one would want a more powerful machine
for learning than for predicting. Decoupling also ensures that failures in
learners do not affect predictors, and vice versa. However, having learners and
predictors together could be interesting in that learning involves predicting,
and thus separating them creates the need to do predictions twice for the same
examples: once in the predictor and once in the learner. In this project, we
abstracted out the learning part, and focused on minimizing the first two terms
in Equation 1. This choice was made assuming that a fixed, small number of
learners would be sufficient for many applications, and that this number would
not need to be changed very often - in contrast to predictors, who would need to
be changed according to load.

\subsection{Load Prediction}
Ideally, we would employ a machine learning algorithm in order to predict the
load in each future time interval. Features that we would consider include:
\begin{enumerate}
\item Time of day: traffic in most applications have load patterns that are very
time-specific. Most people are sleeping at night, have a lunch break around the
middle of the day, etc.
\item Trends in other services by the same company: if Facebook sees an increase
in the number of users in the website at a particular time, it is more likely
that internal services that use machine learning will also see an increase.
\item Usual load pattern: certain applications have very predictable load
patterns. An internal service that always gets called in exponentially growing
bursts would be an example.
\item Trend in the last time interval(s): by measuring the load at smaller
intervals in the last time interval, one can build a ``load trend'' for the next
time interval.
\end{enumerate}
Since we used simulated data (we do not have access to such a rich dataset), we
employed the trend in the last time interval in order to predict future load, by
using traditional linear regression. In order to ensure a smoother decay (and
thus prevent SLA violations due to wrong predictions), we took the max between the upper limit
on a 95\% confidence interval on the linear regression and the number of
requests in the previous time interval as a prediction. Figure \ref{loadsss}
illustrates the behaviour of our predictions on simulated data. We assume that
we must predict the load at every minute, and collect the current load at every
5 seconds in order to build the trend line. The blue lines are the actual load,
while the green and yellow lines indicate the linear regression prediction and
the max we cited before, respectively. It is clear that the yellow line is more
``conservative'', and always stays on top of the actual load. This is what we
used for predicting loads in our experiments.

\begin{figure}[h!]
\centering
\begin{subfigure}{.6\textwidth}
\includegraphics[width=\textwidth]{Smart182predictedVsActual.png}
\caption{Load Pattern 1}
\end{subfigure}
\begin{subfigure}{.6\textwidth}
\includegraphics[width=\textwidth]{Smart282predictedVsActual.png}
\caption{Load Pattern 2}
\end{subfigure}
\begin{subfigure}{.6\textwidth}
\includegraphics[width=\textwidth]{Smart382predictedVsActual.png}
\caption{Load Pattern 3}
\end{subfigure}
\caption{Predicted vs Actual load, different load patterns.}
\label{loadsss}
\end{figure}
\newpage

~
\newpage
\subsection{Strategies}
\subsubsection{Naive Strategy}
The Naive strategy, presented in Algorithm 1, is probably the first solution
anyone would come up with when faced with the auto scaling problem. It is a
reactive strategy that adds more nodes when the SLA is being violated, and
removes nodes when the SLA is far from being violated. In our experiments, we
assumed that the SLA was based on average prediction time being smaller then
some constant $T$, and set $C = T / 2$.

\begin{algorithm}[H]
\If{There is an SLA violation in the current time interval} {
Add more nodes to the system
}
\If{The mean prediction time is smaller than some constant $C$} {
Remove nodes from the system
}
\caption{Naive Strategy}
\end{algorithm}

\subsubsection{Power Strategy}
The Power strategy, depicted in Algorithm 2, takes into account the power (number of requests the machine
can handle) of each node, and compares the sum of the power of all the current
nodes against the number of predicted requests for the next time interval. If
the current power of the system is smaller than the predicted requests, it adds
more nodes. If removing nodes does not cause the power of the system to be
smaller than the number of predicted requests, it removes nodes.

\begin{algorithm}[H]
\While{Current Power $<$ predicted requests} {
Add more nodes to the system
}
\While{Current Power - power($node_i$) $>$ predicted requests} {
Remove $node_i$
}
\caption{Power Strategy}
\end{algorithm}

\subsubsection{Smart Strategy}
The two strategies presented before do not take into account the costs
$\alpha$ and $\beta$. Furthermore, they do not take into account the probability
of nodes failing, or the uncertainty associated with the load prediction. A
reformulated objective function is presented in Equation 2. Here, we are
selecting the configuration of machines $P$ and $L$ that minimizes the expected
cost in the next iteration.

\begin{equation}
min_{P,L}: E[cost] = E[\alpha SLA + \beta P + \gamma LR + \delta L]
\end{equation}

Keeping the learners fixed, as we did before, the reformulated objective
becomes:
\begin{eqnarray*}
min_{P}: E[cost] = E[\alpha SLA + \beta P] \\
  = \alpha E[SLA] + \beta P
\end{eqnarray*}

One way of defining a SLA violation is when we have less power in a time
interval than needed for the number of requests. Ideally, we would integrate out
the uncertainty over our future prediction, and calculate $E[SLA]$ as follows,
given that $\rho$ is the probability that a node fails in the time interval, and
each $r$ denotes a possible value for the future load.
\begin{eqnarray*}
E[SLA] = P(SLA | P, \rho) = \int{P(power < r | P, \rho, r)dr}
\end{eqnarray*}

This would require a more reliable estimator of future load, one that could
output a probability distribution over the possible values of $r$. There are
multiple machine learning algorithms that can do this, but since we are only
using the trend in the last interval as a feature, we did not think it
worthwhile to pursue this route for this project. Therefore, we assumed that the
prediction for the future load was correct. $E[SLA]$ then becomes as following,
noting that $k$ is the minimum number of nodes required for the system's power
to be greater than the number of requests:

\begin{eqnarray*}
P(SLA | P, \rho, requests) = P(power < requests | P, \rho)\\
 = P(|\text{non failing nodes}| < k) \\ \approx CDF Binomial (P, k-1, 1 - \rho)
\end{eqnarray*}

Note that the last approximation depends on the nodes' power being close to
uniform. This is a reasonable assumption on most cases. The smart strategy,
therefore, amounts to picking $P$ such that the expected cost function presented
in Equation 2 is minimized.

\section{Experiments}
For our experiments, we simulated the architecture on Figure 1 on a single
machine, by having multiple servers on different ports, and multiple clients
with different threads. We made the time per prediction be constant, by having
it be a sleep with $t = 0.1$ seconds (so we are not making real predictions,
just simulating them). Finally, we set the time interval on which the autoscaler
works to be one minute - so that the state of the system can potentially change
every minute. The Autoscaler sends heartbeats to the Load Balancer, and finds
out how many requests have been served every 5 seconds, in order to build a
trend for load prediction. The Autoscaler sends heartbeats to the predictors
every 30 seconds, in order to check (1), that they are still up, and (2), that
their power remains the same (if not, it includes the change in the next state
for smart load balancing).

Figures 3, 4 and 5 present the behaviour of the different load balancing
strategies when faced with the loads previously introduced in Figure
\ref{loadsss}. The loads are now depicted with a granularity of 1 second, as
opposed to 1 minute in Figure 2. The blue lines indicate the number of requests,
and the green line indicates either the number of active nodes or whether or not
an SLA violation ocurred. The vertical dotted lines indicate the change points -
that is, the moments when the autoscaler changes (or not) the system's
configuration, which happens every 60 seconds.

In Figures 3(a) and 3(b), we see the behaviour of the Naive strategy. As soon as
SLA violations start to occur, it adds a new node into the system. However, once
the new node is added, the average prediction time goes down dramatically, and
the strategy proceeds to remove a node - which causes a lot of SLA violations,
and thus subsequent additions of nodes. The Power Strategy (Figures 3(c) and
3(d)) does better, as it takes into account the power of the nodes. It is able
to react to the increase in load, although it still suffers two SLA violations
in the beginning. The Smart strategy with high SLA cost $\alpha$ presented in
Figures 3(e) and 3(f) anticipates the
rise in load, and adds enough nodes to handle it before it happens, suffering no
SLA violations. In Figures 3(g) and 3(h), we set the cost of each node to be the
same as the cost of an SLA violation. When this happens, the system has no
incentive to add new nodes, and thus suffers SLA violations throughout the
experiment. This shows how the smart strategy adapts to the costs $\alpha$ and
$\beta$ accordingly.

Figure 4 presentes a pathological case, where the load goes up and down abruptly
right before or right after the change points. The Naive strategy is once again
reactive. The Power strategy suffers the most from this, as it takes nodes up
and down abruptly, and thus suffers the most SLA violations. The Smart strategy
with high $\alpha$ is more conservative throughout, and suffers the least number
of SLA violations.

Finally, Figure 5 presents a load pattern similar to the one in Figure 3, but
duplicated. All strategies do the same as they did in the first load pattern.
However, this figure shows how effective auto-scaling can be at managing the
cost of having multiple nodes: the Smart strategy is able to suffer no SLA
violations, while only using two nodes when the load demands it, and staying
with one node when it doesn't.

\begin{figure}[h!]
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Naive182nodes.png}
\caption{Naive Strategy - active nodes}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Naive182SLA.png}
\caption{Naive Strategy - SLA violations}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Power182nodes.png}
\caption{Power Strategy - active nodes}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Power182SLA.png}
\caption{Power Strategy - SLA violations}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Smart182nodes.png}
\caption{Smart Strategy - active nodes}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Smart182SLA.png}
\caption{Smart Strategy - SLA violations}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Smart188nodes.png}
\caption{Smart Strategy with high node cost - active nodes}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Smart188SLA.png}
\caption{Smart Strategy with high node cost - SLA violations}
\end{subfigure}

\label{load1}
\caption{Load Pattern 1}
\end{figure}

\begin{figure}[h!]
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Naive282nodes.png}
\caption{Naive Strategy - active nodes}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Naive282SLA.png}
\caption{Naive Strategy - SLA violations}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Power282nodes.png}
\caption{Power Strategy - active nodes}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Power282SLA.png}
\caption{Power Strategy - SLA violations}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Smart222nodes.png}
\caption{Smart Strategy - active nodes}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Smart282SLA.png}
\caption{Smart Strategy - SLA violations}
\end{subfigure}

\label{load2}
\caption{Load Pattern 2}
\end{figure}

\begin{figure}[h!]
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Naive382nodes.png}
\caption{Naive Strategy - active nodes}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Naive382SLA.png}
\caption{Naive Strategy - SLA violations}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Power382nodes.png}
\caption{Power Strategy - active nodes}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Power382SLA.png}
\caption{Power Strategy - SLA violations}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Smart382nodes.png}
\caption{Smart Strategy - active nodes}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{Smart382SLA.png}
\caption{Smart Strategy - SLA violations}
\end{subfigure}

\label{load3}
\caption{Load Pattern 3}
\end{figure}

\section{Conclusion}
We have proposed an architecture and strategies for auto scaling online
learning. In our methods and experiments, we abstracted out the online learning
part, and presented and evaluated algorithms that are general to other auto
scaling problems. We propose and evaluate a strategy that takes uncertainty and
node failures into account, by modeling the cost function as an expectation.

As future work, we would like to incorporate the machine learning aspect back
into the system - possibly even coupling together learning and predicting (and
thus modifying the cost function). This is an exciting area that is still new,
and not a lot of research has been done on it.

\clearpage

\bibliographystyle{plain}
\bibliography{references}

\end{document}
