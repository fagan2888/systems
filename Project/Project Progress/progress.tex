\documentclass[letterpaper]{article}
\title{Auto Scaling Online Learning}
\date{}
\usepackage{balance}  % to better equalize the last page
\usepackage{graphicx}
\usepackage{times}    % comment if you want LaTeX's default font
\usepackage{url}      % llt: nicely formatted URLs
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{float}
\usepackage{color}
\usepackage{url}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\usepackage{verbatim}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{subcaption}
%\usepackage{amsmath}
\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{xspace}
\usepackage{multirow}
\setlength{\topmargin}{-.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{.125in}
\setlength{\textwidth}{6.25in}
\setlength{\parskip}{3pt}

\begin{document}
\author{CSE 550 Project Progress \\\\ Marco Tulio Correia Ribeiro, Shrainik Jain\\ 1323300, 1323338}
\maketitle

\section{Progress}
So far we have decided upon an architecture for the Auto Scaling system. For now we will treat the machine learning aspect as a black box, we will use \cite{MachineLearningSystem} for this.
The architecture can we visualized as a set of nodes acting as a learners and predictors. For further simplicity, we assume only one node acting as a learner at a time. Other than this, this is a master load balancing node 
which handles incoming requests from clients and redirects them to learner and predictors as applicable. All the communication between the master node and the other nodes and clients is done via Thrift\footnote{http://thrift.apache.org}. 
\\
The master node or the load balancer has information about all the running nodes and criteria for load balancing based on the weighted sum of load per node. Further, the master also implements a learning algorithm to predict the load. We will using something similar to \cite{LoadPrediction} for load prediction. 
\\
The clients communicate with master directly. As of now, we have 2 possible models in mind for handling incoming requests:
\begin{itemize}
\item Master, upon recieving a request, updates the model for load prediction, requests the learner or predictor as per the client request and relays the response back to client. In this model, the master is a potential bottleneck.
\item Master, upon recieving a request from client, updates the model for load prediction and returns a session and node to client. The client talks to the node directly until the sesion is valid. The master can invalidate the session if the node goes down is overly loaded. After session invalidation, the client contacts master for a new session.
\end{itemize}

Irrespective of the model, the master keeps a list of valid nodes which are up and keeps adding or removing nodes from this list based on load. Master also sends regular heartbeat messages to nodes to check if some node has gone down acts accordingly.


\bibliographystyle{plain}
\bibliography{references}

\end{document}
